{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01: Introduction to Pytorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this task is to get us thinking not just about training models, but about our *training pipelines*.\n",
    "\n",
    "A neural network is a function, $f$, that accepts in data inputs, $\\boldsymbol{X}$, and weights, $\\boldsymbol{\\Theta}$ that produces labels $\\boldsymbol{\\hat{y}}$,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = f(\\Theta; \\boldsymbol{X}).\n",
    "$$\n",
    "\n",
    "Meanwhile, a neural network training process, is itself a function, $g$, which accepts as input a dataset $x$, and for supervised algorithms a set of targets $y$, along with a set of parameters $\\boldsymbol{\\Omega}$ which define how the process is performed, and produces as output the weights of a neural network, $\\boldsymbol{\\Theta}$,\n",
    "\n",
    "$$\n",
    "\\Theta = g(\\boldsymbol{\\Omega}; \\boldsymbol{X}, \\boldsymbol{y}).\n",
    "$$\n",
    "\n",
    "It is helpful to think of the training function, $g$, as a pipeline, composed of several training steps, which can include preprocessing, post processing, etc.\n",
    "\n",
    "$$\n",
    "g = g_N \\circ\\ \\cdots\\ \\circ g_1.\n",
    "$$\n",
    "\n",
    "For example, $g_1$ might be a preprocessing step, then $g_2$ might be a training step, and $g_3$ might be a pruning step in a basic pipeline where data $(\\boldsymbol{X}, \\boldsymbol{y})$ goes in and weights $\\boldsymbol{\\Theta}$ come out.\n",
    "\n",
    "We will learn to think of the training process this way by modifying some example code for a basic MNIST classification task. We begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from dataclasses import dataclass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01 - Part 1\n",
    "\n",
    "Your first task is to:\n",
    "\n",
    "* Add layer definitions to the following neural network class\n",
    "* Define the forward pass\n",
    "\n",
    "You can find starting architectures online. It is important to know there is no known theory to identify a best architecture *before* starting the problem. Trial and error (by iterative training and testing) is the only way to prove or disprove the utility of an architecture.\n",
    "\n",
    "That said, recall some intuition about the way linear and nonlinear transforms work. We know we need a chain of both to have any hope of solving this problem. We also know that we need some depth, and cannot solve this problem by width alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define layers for MNIST classification\n",
    "        self.fc1 = nn.Linear(28 * 28, 128) #   # First fully connected layer (input to hidden layer)\n",
    "        self.fc2 = nn.Linear(128, 64)  # Second fully connected layer (hidden layer to another hidden layer)\n",
    "        self.fc3 = nn.Linear(64, 10)  # Output layer (10 classes for MNIST)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image from 28x28 to a vector of size 784\n",
    "        # Here we define what we do with layers\n",
    "        # We can add dropouts for overfittings. \n",
    "        # Dropout randomly sets a fraction of input units to 0 at each update during training time,\n",
    "        # which helps prevent overfitting.\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout during training [It will randomly set some values to 0]\n",
    "        \n",
    "        # Flatten the input image from 28x28 to a vector of size 784 (which is the input to the first fully connected layer)\n",
    "        # Here we transform the input data from a 2D tensor (image) to a 1D tensor (vector)\n",
    "        # This is because the fully connected layer expects a 1D input.\n",
    "        # The view function reshapes the tensor to have a new shape. 28*28 is the total number of pixels in an image. 784 is the number of neurons in the first fully connected layer. 1 is the batch size, which is not needed for this case.\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # Apply ReLU activation after each layer\n",
    "        x = F.relu(self.fc1(x))  # First hidden layer # Relu is a function \n",
    "        x = F.relu(self.fc2(x))  # Second hidden layer\n",
    "        \n",
    "        # Output layer with log-softmax for classification [It will give me prob for classes for all the 10 classes]\n",
    "        # Then we get the argmax which will give me the max probability of the 10 classes\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)  # Return log probabilities [Here I got the max prob]\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x)\n",
    "    \n",
    "def run_training_epoch(training_params, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % training_params.log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            if training_params.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def predict(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code: Training Pipeline\n",
    "\n",
    "For this assignment, the training pipeline is defined for you. Notice the similarities to the mathematical description of a trainer we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainingParameters:\n",
    "    \"\"\"Training parameters for a simple neural network trainer.\"\"\"\n",
    "\n",
    "    batch_size: int = 64\n",
    "    test_batch_size: int = 1000\n",
    "    epochs: int = 14\n",
    "    lr: float = 1.0\n",
    "    gamma: float = 0.7\n",
    "    normalizer_mean = 0.1307\n",
    "    normalizer_std = 0.3081\n",
    "    no_cuda: bool = True  # Enable or disable CUDA\n",
    "    no_mps: bool = True  # Enable or disable GPU on MacOS\n",
    "    dry_run: bool = False\n",
    "    seed: int = 1\n",
    "    log_interval: int = 10\n",
    "    save_model: bool = True\n",
    "\n",
    "\n",
    "def configure_training_device(training_params):\n",
    "    use_cuda = not training_params.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not training_params.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(training_params.seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {\"batch_size\": training_params.batch_size}\n",
    "    test_kwargs = {\"batch_size\": training_params.test_batch_size}\n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    return device, train_kwargs, test_kwargs\n",
    "\n",
    "\n",
    "def build_preprocessing_transform(training_params):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (training_params.normalizer_mean,),\n",
    "                (training_params.normalizer_std,),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "def build_data_loaders(train_kwargs, test_kwargs, transform):\n",
    "    dataset1 = datasets.MNIST(\n",
    "        \"../data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(training_params, device, train_loader, test_loader):\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=training_params.lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=training_params.gamma)\n",
    "\n",
    "    for epoch in range(1, training_params.epochs + 1):\n",
    "        run_training_epoch(\n",
    "            training_params, model, device, train_loader, optimizer, epoch\n",
    "        )\n",
    "        predict(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        if training_params.save_model:\n",
    "            torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Execute a Training Pipeline\n",
    "\n",
    "With our training steps defined in modular fashion, we can easily define and execute a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/09cvn6lj0y9gv6fl2yjfrd600000gn/T/ipykernel_54762/159220799.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305220\n",
      "\n",
      "Test set: Average loss: 2.3104, Accuracy: 1657/10000 (17%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def execute_training_pipeline():\n",
    "    training_params = TrainingParameters(epochs=1, dry_run=True)\n",
    "    device, train_kwargs, test_kwargs = configure_training_device(\n",
    "        training_params\n",
    "    )\n",
    "    transform = build_preprocessing_transform(training_params)\n",
    "    train_loader, test_loader = build_data_loaders(\n",
    "        train_kwargs, test_kwargs, transform\n",
    "    )\n",
    "    train(training_params, device, train_loader, test_loader)\n",
    "\n",
    "\n",
    "execute_training_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01 - Part 2: Explore Width\n",
    "\n",
    "Using the example above, define a network with a single hidden layer.\n",
    "\n",
    "Modify the trainer to store the train and test errors in a numpy vector.\n",
    "\n",
    "Create a for loop over to iterate through a few different amounts of hidden neurons and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "(It should be hard to get good convergence here - this is part of the exercise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden layer size: 32\n",
      "\n",
      "Test set: Average loss: 0.2250, Accuracy: 9325/10000 (93%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1717, Accuracy: 9483/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1484, Accuracy: 9547/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1376, Accuracy: 9586/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1319, Accuracy: 9596/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1291, Accuracy: 9597/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1287, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1242, Accuracy: 9616/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1266, Accuracy: 9614/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1222, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1236, Accuracy: 9628/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1223, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1208, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1258, Accuracy: 9661/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1277, Accuracy: 9676/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1322, Accuracy: 9670/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1318, Accuracy: 9674/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1390, Accuracy: 9674/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1438, Accuracy: 9668/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1440, Accuracy: 9667/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1523, Accuracy: 9659/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1526, Accuracy: 9659/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1548, Accuracy: 9646/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1655, Accuracy: 9642/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1673, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1635, Accuracy: 9647/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1768, Accuracy: 9629/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1875, Accuracy: 9622/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1920, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1825, Accuracy: 9658/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2004, Accuracy: 9610/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1972, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2068, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2113, Accuracy: 9605/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2132, Accuracy: 9614/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2132, Accuracy: 9620/10000 (96%)\n",
      "\n",
      "Convergence reached at epoch 36 for hidden size 32\n",
      "Training with hidden layer size: 64\n",
      "\n",
      "Test set: Average loss: 0.1991, Accuracy: 9391/10000 (94%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1490, Accuracy: 9534/10000 (95%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1366, Accuracy: 9572/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1199, Accuracy: 9640/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1162, Accuracy: 9649/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1126, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1123, Accuracy: 9670/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1129, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1134, Accuracy: 9684/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1139, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1193, Accuracy: 9686/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1224, Accuracy: 9706/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1272, Accuracy: 9700/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1284, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1340, Accuracy: 9702/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1495, Accuracy: 9686/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1432, Accuracy: 9699/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1514, Accuracy: 9690/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1481, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1452, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1589, Accuracy: 9711/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1662, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1631, Accuracy: 9712/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1753, Accuracy: 9688/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1640, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1845, Accuracy: 9693/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1755, Accuracy: 9709/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1840, Accuracy: 9714/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Modify the Net class to have a single hidden layer\n",
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SingleLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)  # Single hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)  # Output layer (10 classes for MNIST)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input images\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation for the hidden layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return F.log_softmax(x, dim=1)  # Log softmax for classification\n",
    "\n",
    "\n",
    "# Step 2: Modify the training function to stop when the error converges\n",
    "def train_until_convergence(training_params, device, train_loader, test_loader, hidden_size, convergence_threshold=1e-4):\n",
    "    model = SingleLayerNet(hidden_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    prev_train_error = float('inf')\n",
    "    prev_test_error = float('inf')\n",
    "    \n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(training_params.epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        current_train_error = train_loss / len(train_loader)\n",
    "        train_errors.append(current_train_error)  # Store train loss\n",
    "\n",
    "        # Testing phase\n",
    "        test_loss, accuracy = predict_and_log_accuracy(model, device, test_loader)\n",
    "        test_errors.append(test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Check for convergence: stop if the error stops decreasing\n",
    "        if abs(prev_train_error - current_train_error) < convergence_threshold and \\\n",
    "           abs(prev_test_error - test_loss) < convergence_threshold:\n",
    "            print(f\"Convergence reached at epoch {epoch} for hidden size {hidden_size}\")\n",
    "            break\n",
    "        \n",
    "        prev_train_error = current_train_error\n",
    "        prev_test_error = test_loss\n",
    "\n",
    "    # Return the minimum train and test errors achieved during training, and the maximum accuracy\n",
    "    return min(train_errors), min(test_errors), max(accuracies)\n",
    "\n",
    "\n",
    "# Step 3: Explore different hidden layer sizes and train until convergence\n",
    "def explore_width_and_convergence(training_params, device, train_loader, test_loader):\n",
    "    hidden_sizes = [32, 64, 128, 256, 512]  # Different sizes of hidden layers\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    accuracies = []\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        print(f\"Training with hidden layer size: {hidden_size}\")\n",
    "        train_error, test_error, accuracy = train_until_convergence(\n",
    "            training_params, device, train_loader, test_loader, hidden_size\n",
    "        )\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Step 4: Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(hidden_sizes, train_errors, label=\"Train Error\", marker='o')\n",
    "    plt.plot(hidden_sizes, test_errors, label=\"Test Error\", marker='o')\n",
    "    plt.plot(hidden_sizes, accuracies, label=\"Accuracy\", marker='o')\n",
    "    plt.xlabel(\"Hidden Layer Size\")\n",
    "    plt.ylabel(\"Metrics (Error/Accuracy)\")\n",
    "    plt.title(\"Train & Test Errors and Accuracy for Different Hidden Layer Sizes\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict_and_log_accuracy(model, device, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0  # To count the total number of data points\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)  # Forward pass\n",
    "\n",
    "            # Sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "            \n",
    "            # Get the index of the max log-probability (predicted class)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # Count the correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)  # Total number of data points\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100.0 * correct / total\n",
    "\n",
    "    # Print test results for the current epoch\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            total,\n",
    "            accuracy,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return test_loss, accuracy  # Return the loss and accuracy for logging or further analysis\n",
    "\n",
    "\n",
    "# Example: Set up the pipeline and run the experiment\n",
    "@dataclass\n",
    "class TrainingParameters:\n",
    "    \"\"\"Training parameters for a simple neural network trainer.\"\"\"\n",
    "    batch_size: int = 64\n",
    "    test_batch_size: int = 1000\n",
    "    epochs: int = 100\n",
    "    lr: float = 1.0\n",
    "    gamma: float = 0.7\n",
    "    normalizer_mean = 0.1307\n",
    "    normalizer_std = 0.3081\n",
    "    no_cuda: bool = True  # Enable or disable CUDA\n",
    "    no_mps: bool = True  # Enable or disable GPU on MacOS\n",
    "    dry_run: bool = False\n",
    "    seed: int = 1\n",
    "    log_interval: int = 10\n",
    "    save_model: bool = True\n",
    "\n",
    "\n",
    "def configure_training_device(training_params):\n",
    "    use_cuda = not training_params.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not training_params.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(training_params.seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {\"batch_size\": training_params.batch_size}\n",
    "    test_kwargs = {\"batch_size\": training_params.test_batch_size}\n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    return device, train_kwargs, test_kwargs\n",
    "\n",
    "\n",
    "def build_preprocessing_transform(training_params):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (training_params.normalizer_mean,),\n",
    "                (training_params.normalizer_std,),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def build_data_loaders(train_kwargs, test_kwargs, transform):\n",
    "    dataset1 = datasets.MNIST(\n",
    "        \"../data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "training_params = TrainingParameters(epochs=100, dry_run=False)\n",
    "\n",
    "# Execute the pipeline for different hidden layer sizes\n",
    "device, train_kwargs, test_kwargs = configure_training_device(training_params)\n",
    "transform = build_preprocessing_transform(training_params)\n",
    "train_loader, test_loader = build_data_loaders(train_kwargs, test_kwargs, transform)\n",
    "\n",
    "# Call the function to explore the effect of width and convergence\n",
    "explore_width_and_convergence(training_params, device, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 01 - Part 3: Explore Depth\n",
    "\n",
    "Now using the example above, define several networks with increasing numbers of hidden layers (either convolutional or fully connected).\n",
    "\n",
    "As above, create a for loop over to iterate through a few different depths and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "This example should converge much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Modify the Net class to handle multiple hidden layers\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size))  # First layer\n",
    "\n",
    "        # Add intermediate hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_size, output_size))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)  # Create a sequential model with the layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input images\n",
    "        x = self.model(x)  # Apply the stacked layers\n",
    "        return F.log_softmax(x, dim=1)  # Log softmax for classification\n",
    "\n",
    "\n",
    "# Step 2: Modify the training function to handle different depths\n",
    "def train_and_record_errors_depth(training_params, device, train_loader, test_loader, num_layers):\n",
    "    model = MultiLayerNet(28 * 28, 128, 10, num_layers).to(device)  # Fixed hidden size of 128 for simplicity\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for epoch in range(training_params.epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_errors.append(train_loss / len(train_loader))  # Average train loss\n",
    "\n",
    "        # Testing phase using the predict function\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)  # Average test loss\n",
    "        test_errors.append(test_loss)\n",
    "\n",
    "        # Use the predict function to log accuracy\n",
    "        predict(model, device, test_loader)\n",
    "\n",
    "    return min(train_errors), min(test_errors)  # Return minimum train and test errors\n",
    "\n",
    "\n",
    "# Step 3: Explore different depths (number of hidden layers)\n",
    "def explore_depth(training_params, device, train_loader, test_loader):\n",
    "    num_layers_list = [1, 2, 3, 4, 5]  # Different depths (number of hidden layers)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for num_layers in num_layers_list:\n",
    "        print(f\"Training with {num_layers} hidden layers\")\n",
    "        train_error, test_error = train_and_record_errors_depth(\n",
    "            training_params, device, train_loader, test_loader, num_layers\n",
    "        )\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    # Step 4: Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(num_layers_list, train_errors, label=\"Train Error\")\n",
    "    plt.plot(num_layers_list, test_errors, label=\"Test Error\")\n",
    "    plt.xlabel(\"Number of Hidden Layers\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(\"Train and Test Errors for Different Network Depths\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "training_params = TrainingParameters(epochs=10, dry_run=False)\n",
    "\n",
    "# Execute the pipeline for different network depths\n",
    "device, train_kwargs, test_kwargs = configure_training_device(training_params)\n",
    "transform = build_preprocessing_transform(training_params)\n",
    "train_loader, test_loader = build_data_loaders(train_kwargs, test_kwargs, transform)\n",
    "\n",
    "# Call the function to explore the effect of depth\n",
    "explore_depth(training_params, device, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
