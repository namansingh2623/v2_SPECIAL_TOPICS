{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01: Introduction to Pytorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this task is to get us thinking not just about training models, but about our *training pipelines*.\n",
    "\n",
    "A neural network is a function, $f$, that accepts in data inputs, $\\boldsymbol{X}$, and weights, $\\boldsymbol{\\Theta}$ that produces labels $\\boldsymbol{\\hat{y}}$,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = f(\\Theta; \\boldsymbol{X}).\n",
    "$$\n",
    "\n",
    "Meanwhile, a neural network training process, is itself a function, $g$, which accepts as input a dataset $x$, and for supervised algorithms a set of targets $y$, along with a set of parameters $\\boldsymbol{\\Omega}$ which define how the process is performed, and produces as output the weights of a neural network, $\\boldsymbol{\\Theta}$,\n",
    "\n",
    "$$\n",
    "\\Theta = g(\\boldsymbol{\\Omega}; \\boldsymbol{X}, \\boldsymbol{y}).\n",
    "$$\n",
    "\n",
    "It is helpful to think of the training function, $g$, as a pipeline, composed of several training steps, which can include preprocessing, post processing, etc.\n",
    "\n",
    "$$\n",
    "g = g_N \\circ\\ \\cdots\\ \\circ g_1.\n",
    "$$\n",
    "\n",
    "For example, $g_1$ might be a preprocessing step, then $g_2$ might be a training step, and $g_3$ might be a pruning step in a basic pipeline where data $(\\boldsymbol{X}, \\boldsymbol{y})$ goes in and weights $\\boldsymbol{\\Theta}$ come out.\n",
    "\n",
    "We will learn to think of the training process this way by modifying some example code for a basic MNIST classification task. We begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01 - Part 1\n",
    "\n",
    "Your first task is to:\n",
    "\n",
    "* Add layer definitions to the following neural network class\n",
    "* Define the forward pass\n",
    "\n",
    "You can find starting architectures online. It is important to know there is no known theory to identify a best architecture *before* starting the problem. Trial and error (by iterative training and testing) is the only way to prove or disprove the utility of an architecture.\n",
    "\n",
    "That said, recall some intuition about the way linear and nonlinear transforms work. We know we need a chain of both to have any hope of solving this problem. We also know that we need some depth, and cannot solve this problem by width alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define layers for MNIST classification\n",
    "        self.fc1 = nn.Linear(28 * 28, 128) #   # First fully connected layer (input to hidden layer)\n",
    "        self.fc2 = nn.Linear(128, 64)  # Second fully connected layer (hidden layer to another hidden layer)\n",
    "        self.fc3 = nn.Linear(64, 10)  # Output layer (10 classes for MNIST)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input image from 28x28 to a vector of size 784\n",
    "        # Here we define what we do with layers\n",
    "        # We can add dropouts for overfittings. \n",
    "        # Dropout randomly sets a fraction of input units to 0 at each update during training time,\n",
    "        # which helps prevent overfitting.\n",
    "        # x = F.dropout(x, training=self.training)  # Apply dropout during training [It will randomly set some values to 0]\n",
    "        \n",
    "        # Flatten the input image from 28x28 to a vector of size 784 (which is the input to the first fully connected layer)\n",
    "        # Here we transform the input data from a 2D tensor (image) to a 1D tensor (vector)\n",
    "        # This is because the fully connected layer expects a 1D input.\n",
    "        # The view function reshapes the tensor to have a new shape. 28*28 is the total number of pixels in an image. 784 is the number of neurons in the first fully connected layer. 1 is the batch size, which is not needed for this case.\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # Apply ReLU activation after each layer\n",
    "        x = F.relu(self.fc1(x))  # First hidden layer # Relu is a function \n",
    "        x = F.relu(self.fc2(x))  # Second hidden layer\n",
    "        \n",
    "        # Output layer with log-softmax for classification [It will give me prob for classes for all the 10 classes]\n",
    "        # Then we get the argmax which will give me the max probability of the 10 classes\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)  # Return log probabilities [Here I got the max prob]\n",
    "\n",
    "\n",
    "def run_training_epoch(training_params, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % training_params.log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            if training_params.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def predict(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    # Ensure that test_loss is appended to the model's test_errors list\n",
    "\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code: Training Pipeline\n",
    "\n",
    "For this assignment, the training pipeline is defined for you. Notice the similarities to the mathematical description of a trainer we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingParameters:\n",
    "    \"\"\"Training parameters for a simple neural network trainer.\"\"\"\n",
    "\n",
    "    batch_size: int = 64\n",
    "    test_batch_size: int = 1000\n",
    "    epochs: int = 14\n",
    "    lr: float = 1.0\n",
    "    gamma: float = 0.7\n",
    "    normalizer_mean = 0.1307\n",
    "    normalizer_std = 0.3081\n",
    "    no_cuda: bool = True  # Enable or disable CUDA\n",
    "    no_mps: bool = True  # Enable or disable GPU on MacOS\n",
    "    dry_run: bool = False\n",
    "    seed: int = 1\n",
    "    log_interval: int = 10\n",
    "    save_model: bool = True\n",
    "\n",
    "\n",
    "def configure_training_device(training_params):\n",
    "    use_cuda = not training_params.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not training_params.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(training_params.seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {\"batch_size\": training_params.batch_size}\n",
    "    test_kwargs = {\"batch_size\": training_params.test_batch_size}\n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    return device, train_kwargs, test_kwargs\n",
    "\n",
    "\n",
    "def build_preprocessing_transform(training_params):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (training_params.normalizer_mean,),\n",
    "                (training_params.normalizer_std,),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "def build_data_loaders(train_kwargs, test_kwargs, transform):\n",
    "    dataset1 = datasets.MNIST(\n",
    "        \"../data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(training_params, device, train_loader, test_loader):\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=training_params.lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=training_params.gamma)\n",
    "\n",
    "    for epoch in range(1, training_params.epochs + 1):\n",
    "        run_training_epoch(\n",
    "            training_params, model, device, train_loader, optimizer, epoch\n",
    "        )\n",
    "        predict(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        if training_params.save_model:\n",
    "            torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Execute a Training Pipeline\n",
    "\n",
    "With our training steps defined in modular fashion, we can easily define and execute a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304324\n",
      "\n",
      "Test set: Average loss: 2.1982, Accuracy: 2845/10000 (28%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def execute_training_pipeline():\n",
    "    training_params = TrainingParameters(epochs=1, dry_run=True)\n",
    "    device, train_kwargs, test_kwargs = configure_training_device(\n",
    "        training_params\n",
    "    )\n",
    "    transform = build_preprocessing_transform(training_params)\n",
    "    train_loader, test_loader = build_data_loaders(\n",
    "        train_kwargs, test_kwargs, transform\n",
    "    )\n",
    "    train(training_params, device, train_loader, test_loader)\n",
    "\n",
    "\n",
    "execute_training_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01 - Part 2: Explore Width\n",
    "\n",
    "Using the example above, define a network with a single hidden layer.\n",
    "\n",
    "Modify the trainer to store the train and test errors in a numpy vector.\n",
    "\n",
    "Create a for loop over to iterate through a few different amounts of hidden neurons and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "(It should be hard to get good convergence here - this is part of the exercise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import numpy as np\n",
    "\n",
    "class NetWithErrors(Net):\n",
    "    def __init__(self):\n",
    "        super(NetWithErrors, self).__init__()\n",
    "        self.train_errors = []\n",
    "        self.test_errors = []\n",
    "\n",
    "    def run_training_epoch(self, training_params, model, device, train_loader, optimizer, epoch):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate batch loss for the epoch\n",
    "\n",
    "            if batch_idx % training_params.log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "                if training_params.dry_run:\n",
    "                    break\n",
    "\n",
    "        # Store the average training loss for this epoch\n",
    "        self.train_errors.append(epoch_loss / len(train_loader.dataset))\n",
    "\n",
    "    def predict(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        # Ensure this appends the test loss\n",
    "        model.test_errors.append(test_loss)\n",
    "\n",
    "        print(\n",
    "            \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "                test_loss,\n",
    "                correct,\n",
    "                len(test_loader.dataset),\n",
    "                100.0 * correct / len(test_loader.dataset),\n",
    "            )\n",
    "        )\n",
    "# When initializing the model, use the new class NetWithErrors\n",
    "model_with_errors = NetWithErrors()\n",
    "\n",
    "# The rest of the training pipeline will remain the same,\n",
    "# and you can access the errors after training:\n",
    "# e.g., model_with_errors.train_errors and model_with_errors.test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 16 hidden neurons\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.291744\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.611259\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.320696\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.788864\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.476619\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.495227\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.281666\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.349652\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.738747\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.273978\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.261302\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.293486\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.336203\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.250630\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.281069\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.197109\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.490305\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.166250\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.586521\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.322644\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.260535\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.173913\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.250750\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.476452\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.253151\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.660541\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.419605\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.206989\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.277474\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.358959\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.373598\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.232954\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.133125\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.247317\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.094188\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.253049\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.329382\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.405425\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.142804\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.264327\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.167520\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.281700\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.336140\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.244152\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.211518\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.180683\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.202296\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.346793\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.273823\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.452865\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.311177\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.258107\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.256963\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.099735\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.187896\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.274803\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.167707\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.229301\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.325842\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.183520\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.204822\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.115641\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.168720\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.195758\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.288226\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.196291\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.123414\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.430997\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.352208\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.176256\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.337617\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.393134\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.289191\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.260115\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.212663\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.175330\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.179740\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.174576\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.152187\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.313357\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.240745\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.146651\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.063375\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.233871\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.136518\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.183228\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.127395\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.376595\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.126716\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.229398\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.191938\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.082822\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.045568\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.036827\n",
      "\n",
      "Test set: Average loss: 0.2050, Accuracy: 9367/10000 (94%)\n",
      "\n",
      "Epoch 1: model.test_errors = []\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "model.test_errors is empty after calling predict().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m transform \u001b[38;5;241m=\u001b[39m build_preprocessing_transform(training_params)\n\u001b[1;32m     64\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m build_data_loaders(train_kwargs, test_kwargs, transform)\n\u001b[0;32m---> 66\u001b[0m hidden_neurons_list, min_errors \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_until_convergence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_neurons_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[1;32m     69\u001b[0m plot_errors(hidden_neurons_list, min_errors)\n",
      "Cell \u001b[0;32mIn[22], line 32\u001b[0m, in \u001b[0;36mtrain_until_convergence\u001b[0;34m(training_params, hidden_neurons_list)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Check if the list is still empty\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mtest_errors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.test_errors is empty after calling predict().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m current_test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtest_errors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Stop training if the error stops decreasing (convergence condition)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: model.test_errors is empty after calling predict()."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class NetVariableHiddenLayers(Net):\n",
    "    def __init__(self, hidden_neurons):\n",
    "        super(NetVariableHiddenLayers, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_neurons)\n",
    "        self.fc2 = nn.Linear(hidden_neurons, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.train_errors = []\n",
    "        self.test_errors = []  # This initializes the list\n",
    "\n",
    "        \n",
    "# Now use the external `run_training_epoch` and `predict` functions as before\n",
    "def train_until_convergence(training_params, hidden_neurons_list):\n",
    "    min_errors = []\n",
    "    \n",
    "    for hidden_neurons in hidden_neurons_list:\n",
    "        print(f\"\\nTraining with {hidden_neurons} hidden neurons\")\n",
    "        model = NetVariableHiddenLayers(hidden_neurons).to(device)\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=training_params.lr)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=training_params.gamma)\n",
    "\n",
    "        prev_test_loss = float('inf')  # Initialize to infinity\n",
    "        for epoch in range(1, training_params.epochs + 1):\n",
    "            run_training_epoch(training_params, model, device, train_loader, optimizer, epoch)\n",
    "            predict(model, device, test_loader)\n",
    "            \n",
    "            # Debugging: print the contents of test_errors\n",
    "            print(f\"Epoch {epoch}: model.test_errors = {model.test_errors}\")\n",
    "            \n",
    "            # Check if the list is still empty\n",
    "            if len(model.test_errors) == 0:\n",
    "                raise RuntimeError(\"model.test_errors is empty after calling predict().\")\n",
    "\n",
    "            current_test_loss = model.test_errors[-1]\n",
    "\n",
    "            # Stop training if the error stops decreasing (convergence condition)\n",
    "            if abs(prev_test_loss - current_test_loss) < 1e-2:\n",
    "                print(f\"Convergence reached at epoch {epoch} for {hidden_neurons} neurons.\")\n",
    "                break\n",
    "            prev_test_loss = current_test_loss\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Store the minimum error achieved for this number of neurons\n",
    "        min_errors.append(min(model.test_errors))\n",
    "    \n",
    "    return hidden_neurons_list, min_errors\n",
    "# Plotting function\n",
    "def plot_errors(hidden_neurons_list, min_errors):\n",
    "    plt.figure()\n",
    "    plt.plot(hidden_neurons_list, min_errors, marker='o')\n",
    "    plt.xlabel(\"Number of Hidden Neurons\")\n",
    "    plt.ylabel(\"Minimum Test Error\")\n",
    "    plt.title(\"Minimum Test Error vs Number of Hidden Neurons\")\n",
    "    plt.show()\n",
    "\n",
    "# Example of hidden neuron list to try (you can change these values)\n",
    "hidden_neurons_list = [16, 32, 64, 128]\n",
    "\n",
    "# Execute training with different hidden neuron sizes\n",
    "training_params = TrainingParameters(epochs=100, dry_run=False)  # Set epochs high, training will stop on convergence\n",
    "device, train_kwargs, test_kwargs = configure_training_device(training_params)\n",
    "transform = build_preprocessing_transform(training_params)\n",
    "train_loader, test_loader = build_data_loaders(train_kwargs, test_kwargs, transform)\n",
    "\n",
    "hidden_neurons_list, min_errors = train_until_convergence(training_params, hidden_neurons_list)\n",
    "\n",
    "# Plot the results\n",
    "plot_errors(hidden_neurons_list, min_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 01 - Part 3: Explore Depth\n",
    "\n",
    "Now using the example above, define several networks with increasing numbers of hidden layers (either convolutional or fully connected).\n",
    "\n",
    "As above, create a for loop over to iterate through a few different depths and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "This example should converge much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
